{"episode_reward": 0.0, "episode": 1.0, "duration": 0.2898557186126709, "step": 500}
{"episode_reward": 162.61399936458687, "episode": 5.0, "duration": 0.29108357429504395, "step": 1000}
{"episode_reward": 77.55861797884202, "episode": 9.0, "batch_reward": 1.06127188205719, "critic_loss": 3.336524152755737, "actor_loss": -2.153484511375427, "actor_target_entropy": -1.0, "actor_entropy": 0.4947023630142212, "alpha_loss": 0.09926893711090087, "alpha_value": 0.09906704321565565, "curl_loss": 3.8981924057006836, "duration": 10.140615940093994, "step": 1500}
{"episode_reward": 134.88092704211513, "episode": 13.0, "batch_reward": 1.1034109115600585, "critic_loss": 4.873368501663208, "actor_loss": -4.223454475402832, "actor_target_entropy": -1.0, "actor_entropy": 0.9711791634559631, "alpha_loss": 0.1482480674982071, "alpha_value": 0.09668884856026486, "curl_loss": 2.3746246814727785, "duration": 9.603301763534546, "step": 2000}
{"episode_reward": 149.31074165397052, "episode": 17.0, "batch_reward": 1.1212876081466674, "critic_loss": 4.866904354095459, "actor_loss": -5.735430908203125, "actor_target_entropy": -1.0, "actor_entropy": 0.702005136013031, "alpha_loss": 0.13308898508548736, "alpha_value": 0.09451063440152517, "curl_loss": 1.6039629697799682, "duration": 9.479459285736084, "step": 2500}
{"episode_reward": 193.3414508196435, "episode": 21.0, "batch_reward": 1.1646197199821473, "critic_loss": 6.121649932861328, "actor_loss": -7.774649620056152, "actor_target_entropy": -1.0, "actor_entropy": 0.7443809866905212, "alpha_loss": 0.12600673139095306, "alpha_value": 0.09245051537634492, "curl_loss": 1.4247175455093384, "duration": 9.411666870117188, "step": 3000}
{"episode_reward": 152.33606798021614, "episode": 25.0, "batch_reward": 1.289945363998413, "critic_loss": 8.557127380371094, "actor_loss": -10.449567794799805, "actor_target_entropy": -1.0, "actor_entropy": 0.5775108098983764, "alpha_loss": 0.10886925011873246, "alpha_value": 0.09048339181586805, "curl_loss": 1.2039543628692626, "duration": 9.436804294586182, "step": 3500}
{"episode_reward": 222.23702672698897, "episode": 29.0, "batch_reward": 1.2655027866363526, "critic_loss": 9.88310956954956, "actor_loss": -12.853021049499512, "actor_target_entropy": -1.0, "actor_entropy": 0.8672703266143799, "alpha_loss": 0.10495777279138566, "alpha_value": 0.08876246773593004, "curl_loss": 1.216940689086914, "duration": 9.433341264724731, "step": 4000}
{"episode_reward": 261.7666529908629, "episode": 33.0, "batch_reward": 1.3652334928512573, "critic_loss": 5.250934982299805, "actor_loss": -14.983049583435058, "actor_target_entropy": -1.0, "actor_entropy": 0.6802450299263001, "alpha_loss": 0.08837809860706329, "alpha_value": 0.08706033695001934, "curl_loss": 1.143353319168091, "duration": 9.465448141098022, "step": 4500}
{"episode_reward": 224.38633611630308, "episode": 37.0, "batch_reward": 1.338538908958435, "critic_loss": 10.097601127624511, "actor_loss": -17.729771041870116, "actor_target_entropy": -1.0, "actor_entropy": 0.7211209058761596, "alpha_loss": 0.05730616077780724, "alpha_value": 0.08570135118769198, "curl_loss": 1.025518250465393, "duration": 9.410716772079468, "step": 5000}
{"episode_reward": 209.97056834070193, "episode": 41.0, "batch_reward": 1.4584504127502442, "critic_loss": 11.31536636352539, "actor_loss": -21.42230339050293, "actor_target_entropy": -1.0, "actor_entropy": 0.6623682141304016, "alpha_loss": 0.05160022228956222, "alpha_value": 0.08461424068344128, "curl_loss": 0.8077561020851135, "duration": 9.442504405975342, "step": 5500}
{"episode_reward": 197.01326751976333, "episode": 45.0, "batch_reward": 1.4993111848831178, "critic_loss": 17.36131763458252, "actor_loss": -24.70911064147949, "actor_target_entropy": -1.0, "actor_entropy": 0.5680656850337982, "alpha_loss": 0.043801095336675644, "alpha_value": 0.08356902255149135, "curl_loss": 0.8971593022346497, "duration": 9.436532735824585, "step": 6000}
{"episode_reward": 224.42217158582793, "episode": 49.0, "batch_reward": 1.4440719842910767, "critic_loss": 17.855226516723633, "actor_loss": -27.544480895996095, "actor_target_entropy": -1.0, "actor_entropy": 0.5956185400485993, "alpha_loss": 0.03507559634745121, "alpha_value": 0.0826286085582129, "curl_loss": 0.7161830186843872, "duration": 9.40358591079712, "step": 6500}
